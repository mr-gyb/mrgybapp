require('dotenv').config();
const express = require('express');
const multer = require('multer');
const cors = require('cors');
const { randomUUID } = require('crypto');
const https = require('https');

// Use native fetch if available (Node 18+), otherwise use node-fetch
let fetch;
let FormData;
try {
  // Try to use native fetch (Node 18+)
  if (globalThis.fetch) {
    fetch = globalThis.fetch;
    FormData = globalThis.FormData;
    console.log('‚úÖ Using native fetch and FormData (Node 18+)');
  } else {
    fetch = require('node-fetch');
    FormData = require('form-data');
    console.log('‚úÖ Using node-fetch and form-data');
  }
} catch (e) {
  // Fallback to node-fetch if native fetch not available
  fetch = require('node-fetch');
  FormData = require('form-data');
  console.log('‚úÖ Using node-fetch and form-data (fallback)');
}

const REQUIRED_ENV = ['OPENAI_API_KEY', 'CHAT_API_BASE', 'MODEL_NAME', 'PORT'];
const missingEnv = REQUIRED_ENV.filter(name => !process.env[name]);

if (missingEnv.length > 0) {
  console.error(
    `[startup] Missing required environment variables: ${missingEnv.join(', ')}`
  );
  process.exit(1);
}

const app = express();
const PORT = parseInt(process.env.PORT, 10);
const DEFAULT_CHAT_MODEL =
  process.env.MODEL_NAME || process.env.OPENAI_MODEL_NAME || 'gpt-4o-mini';
const FALLBACK_MODEL = process.env.FALLBACK_MODEL || 'gpt-4o-mini';
const OPENAI_BASE_URL = process.env.CHAT_API_BASE.replace(/\/$/, '');
const CHAT_TIMEOUT_MS = parseInt(process.env.OPENAI_CHAT_TIMEOUT_MS || '30000', 10);
const ENABLE_MODEL_FALLBACK = process.env.ENABLE_MODEL_FALLBACK !== 'false';

const safeJsonParse = (input) => {
  try {
    return JSON.parse(input);
  } catch {
    return null;
  }
};

const formatErrorResponse = ({ code, status, source, detail, requestId, meta }) => ({
  success: false,
  error: {
    code,
    status,
    source,
    detail,
    requestId,
    meta,
    message: detail,
  },
});

const buildSystemPrompt = (agent = 'Mr.GYB AI') => {
  const prompts = {
    'Mr.GYB AI': 'You are Mr.GYB AI, an all-in-one business growth assistant. You specialize in digital marketing, content creation, and business strategy. Be professional, strategic, and focused on growth. When asked about your name, respond naturally and politely.',
    'Chris': 'You are Chris, the CEO AI, focused on high-level strategic planning and business development. Provide executive-level insights and leadership guidance. When asked about your name, respond naturally and politely.',
    'CHRIS': 'You are Chris, the CEO AI, focused on high-level strategic planning and business development. Provide executive-level insights and leadership guidance. When asked about your name, respond naturally and politely.',
    'Sherry': 'You are Sherry, the COO AI, specializing in operations management and process optimization. Focus on efficiency, systems, and operational excellence. When asked about your name, respond naturally and politely.',
    'Charlotte': 'You are Charlotte, the CHRO AI, expert in human resources and organizational development. Focus on talent management, culture, and employee experience. When asked about your name, respond naturally and politely.',
    'Jake': 'You are Jake, the CTO AI, specializing in technology strategy and innovation. Provide guidance on technical decisions and digital transformation. When asked about your name, respond naturally and politely.',
    'Rachel': 'You are Rachel, the CMO AI, expert in marketing strategy and brand development. Focus on marketing campaigns, brand building, and customer engagement. When asked about your name, respond naturally and politely.'
  };

  return prompts[agent] || 'You are a helpful AI assistant. Be professional and concise in your responses. When asked about your name, respond naturally and politely.';
};

// Middleware
const allowedOrigins = [
  'http://localhost:3000',
  'http://localhost:3001',
  'http://localhost:3002',
  'http://localhost:3003',
  'http://localhost:5173',
  process.env.PROD_ORIGIN,
].filter(Boolean);

app.use(
  cors({
    origin: allowedOrigins,
    credentials: false,
    methods: ['GET', 'POST', 'OPTIONS'],
    allowedHeaders: ['Content-Type', 'Authorization'],
  })
);
app.use(express.json({ limit: '1mb' }));

// Basic health route for connectivity checks
app.get('/health', (_req, res) => {
  res.setHeader('Access-Control-Allow-Origin', _req.headers.origin || '*');
  res.json({
    status: 'ok',
    timestamp: new Date().toISOString(),
  });
});

// Configure multer for file uploads
// Note: Whisper API accepts files up to 25MB, but we allow larger uploads
// and will handle size validation in the endpoint
const upload = multer({
  storage: multer.memoryStorage(),
  limits: {
    fileSize: 500 * 1024 * 1024 // 500MB limit for initial upload (we'll validate Whisper's 25MB limit in endpoint)
  }
});

// Health check endpoint
app.get('/api/transcribe/health', (req, res) => {
  res.json({
    status: 'healthy',
    service: 'transcription',
    openai_configured: !!process.env.OPENAI_API_KEY
  });
});

// Transcription endpoint (accepts both audio and video files)
app.post('/api/transcribe', upload.single('audio'), async (req, res) => {
  try {
    console.log('üé§ Received transcription request');

    if (!req.file) {
      return res.status(400).json({ error: 'No audio/video file provided' });
    }

    // Whisper API accepts both audio and video files up to 25MB
    // Video files are accepted and Whisper will extract audio automatically
    const WHISPER_MAX_SIZE = 25 * 1024 * 1024; // 25MB - Whisper API limit
    if (req.file.size > WHISPER_MAX_SIZE) {
      return res.status(413).json({ 
        error: `File too large for Whisper API. Maximum size is 25MB. Your file is ${(req.file.size / (1024 * 1024)).toFixed(2)}MB. Please compress or trim your video.` 
      });
    }

    if (!process.env.OPENAI_API_KEY) {
      return res.status(500).json({ error: 'OpenAI API key not configured' });
    }

    console.log('üìä Processing audio file:', {
      originalname: req.file.originalname,
      size: req.file.size,
      mimetype: req.file.mimetype
    });

    // Create FormData - use native FormData if available, otherwise use form-data package
    const formData = new FormData();
    const filename = req.file.originalname || 
      (req.file.mimetype?.startsWith('video/') ? 'video.mp4' : 'audio.webm');
    
    // Append file - handle both native FormData and form-data package
    if (FormData.prototype.append.length === 2) {
      // Native FormData (takes Blob/File directly)
      const blob = new Blob([req.file.buffer], { type: req.file.mimetype });
      const file = new File([blob], filename, { type: req.file.mimetype });
      formData.append('file', file);
    } else {
      // form-data package (takes buffer with options)
      formData.append('file', req.file.buffer, {
        filename: filename,
        contentType: req.file.mimetype
      });
    }
    
    formData.append('model', 'whisper-1');
    formData.append('language', 'en');
    formData.append('response_format', 'json');
    formData.append('temperature', '0.0');

    console.log('ü§ñ Sending to OpenAI Whisper API...');

    // Create timeout
    const timeout = 300000; // 5 minutes
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), timeout);

    let response;
    try {
      // Prepare headers
      const headers = {
        'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,
      };
      
      // Add FormData headers if using form-data package (not native)
      if (formData.getHeaders) {
        Object.assign(headers, formData.getHeaders());
      }
      
      const fetchOptions = {
        method: 'POST',
        headers: headers,
        body: formData,
        signal: controller.signal,
      };
      
      // For node-fetch v2, add https agent for better SSL handling
      if (typeof fetch !== 'function' || fetch.toString().includes('node-fetch')) {
        const httpsAgent = new https.Agent({
          keepAlive: true,
          rejectUnauthorized: true,
          // Add SSL options to prevent bad record mac errors
          secureProtocol: 'TLSv1_2_method',
        });
        fetchOptions.agent = httpsAgent;
      }
      
      response = await fetch('https://api.openai.com/v1/audio/transcriptions', fetchOptions);
      
      clearTimeout(timeoutId);
      
      console.log('üì° OpenAI API response status:', response.status);
    } catch (fetchError) {
      clearTimeout(timeoutId);
      console.error('‚ùå Fetch error calling OpenAI:', fetchError);
      
      // Handle specific error types
      if (fetchError.name === 'AbortError' || fetchError.message?.includes('aborted')) {
        throw new Error('Request timed out. The file may be too large or OpenAI API is slow.');
      } else if (fetchError.message && (fetchError.message.includes('SSL') || fetchError.message.includes('TLS') || fetchError.message.includes('bad record mac'))) {
        throw new Error('SSL/TLS error connecting to OpenAI. This may be a network or SSL configuration issue. Please try again.');
      } else {
        throw new Error(`Network error calling OpenAI API: ${fetchError.message}`);
      }
    }

    if (!response.ok) {
      const errorText = await response.text();
      const parsed = safeJsonParse(errorText);
      console.error('‚ùå OpenAI API error:', errorText);

      // Check for quota error
      if (isQuotaError(response.status, parsed)) {
        const retryAfter = extractRetryAfter(response, parsed);
        console.error('[transcribe] quota error detected', {
          status: response.status,
          errorType: parsed?.error?.type,
          errorCode: parsed?.error?.code,
          retryAfter,
        });
        
        return res.status(response.status).json({
          ok: false,
          errorType: 'quota',
          status: response.status,
          error: 'OpenAI API quota exceeded. Please try again later.',
          retryAfter,
        });
      }

      let errorMessage = 'Transcription failed';
      if (response.status === 401) {
        errorMessage = 'OpenAI API key invalid or expired';
      } else if (response.status === 429) {
        errorMessage = 'OpenAI API quota exceeded. Please try again later.';
      } else if (response.status === 413) {
        errorMessage = 'Audio file too large for processing';
      } else {
        errorMessage = `OpenAI API error: ${response.status}`;
      }

      return res.status(response.status).json({ error: errorMessage });
    }

    const result = await response.json();
    console.log('‚úÖ Transcription successful:', result.text);

    res.json({
      text: result.text,
      success: true
    });

  } catch (error) {
    console.error('‚ùå Transcription error:', error);
    
    // Provide more helpful error messages
    let errorMessage = error.message || 'Unknown error';
    
    // Check for SSL/TLS errors
    if (error.message && (error.message.includes('SSL') || error.message.includes('TLS') || error.message.includes('bad record mac'))) {
      errorMessage = 'Network/SSL error connecting to OpenAI. Please check your internet connection and try again. If the problem persists, there may be a network configuration issue.';
    } else if (error.message && error.message.includes('timeout')) {
      errorMessage = 'Request timed out. The video file may be too large or OpenAI API is slow. Please try again with a smaller file.';
    } else if (error.message && error.message.includes('ENOTFOUND') || error.message.includes('ECONNREFUSED')) {
      errorMessage = 'Cannot connect to OpenAI API. Please check your internet connection and OpenAI API status.';
    }
    
    res.status(500).json({
      error: `Transcription failed: ${errorMessage}`
    });
  }
});

app.get('/api/chat/health', async (req, res) => {
  const requestId = randomUUID();

  if (!process.env.OPENAI_API_KEY) {
    return res.status(500).json(
      formatErrorResponse({
        code: 'missing_api_key',
        status: 500,
        source: 'proxy',
        detail: 'OpenAI API key is not configured on the server.',
        requestId,
        meta: { hint: 'Set OPENAI_API_KEY in backend environment.' },
      })
    );
  }

  const payload = {
    model: DEFAULT_CHAT_MODEL,
    messages: [
      { role: 'system', content: 'You are a health-check assistant that only replies with "pong".' },
      { role: 'user', content: 'ping' },
    ],
    max_tokens: 1,
    temperature: 0,
  };

  const start = Date.now();
  try {
    const response = await fetch(`${OPENAI_BASE_URL}/chat/completions`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        Authorization: `Bearer ${process.env.OPENAI_API_KEY}`,
        ...(process.env.OPENAI_ORG ? { 'OpenAI-Organization': process.env.OPENAI_ORG } : {}),
      },
      body: JSON.stringify(payload),
    });
    const latencyMs = Date.now() - start;
    const bodyText = await response.text();

    if (!response.ok) {
      const parsed = safeJsonParse(bodyText);
      const detail =
        parsed?.error?.message ||
        parsed?.message ||
        bodyText.slice(0, 200) ||
        `OpenAI API error (${response.status})`;
      return res.status(response.status).json(
        formatErrorResponse({
          code:
            response.status === 401
              ? 'unauthorized'
              : response.status === 429
              ? 'rate_limited'
              : response.status >= 500
              ? 'upstream_error'
              : 'bad_response',
          status: response.status,
          source: 'openai',
          detail,
          requestId,
          meta: {
            latencyMs,
            raw: parsed ?? bodyText.slice(0, 500),
          },
        })
      );
    }

    const json = safeJsonParse(bodyText);
    const completion = json?.choices?.[0]?.message?.content || 'pong';

    return res.json({
      success: true,
      data: {
        pong: completion,
        latencyMs,
        model: DEFAULT_CHAT_MODEL,
        requestId,
      },
    });
  } catch (error) {
    console.error(`[chat-health:${requestId}] error`, error);
    return res.status(504).json(
      formatErrorResponse({
        code: 'health_check_failed',
        status: 504,
        source: 'network',
        detail: error.message || 'Failed to reach OpenAI during health check.',
        requestId,
      })
    );
  }
});

const logChat = (level, requestId, message, meta = {}) => {
  const timestamp = new Date().toISOString();
  const payload = { requestId, timestamp, ...meta };
  const prefix = `[chat] ${message}`;
  if (level === 'error') {
    console.error(prefix, payload);
  } else if (level === 'warn') {
    console.warn(prefix, payload);
  } else {
    console.info(prefix, payload);
  }
};

/**
 * Detects if an OpenAI error is quota-related (billing/quota exceeded)
 * Based on OpenAI API error codes: https://platform.openai.com/docs/guides/error-codes/api-errors
 */
const isQuotaError = (status, errorData) => {
  const errorType = errorData?.error?.type || errorData?.error?.code;
  const errorCode = errorData?.error?.code;
  
  // Official OpenAI error codes for quota issues
  if (errorType === 'insufficient_quota') return true;
  if (errorCode === 'insufficient_quota') return true;
  
  // Check status code (429 can be either quota or rate limit)
  if (status === 429 && errorType === 'insufficient_quota') return true;
  
  // Check error message for quota-related keywords
  const message = (errorData?.error?.message || errorData?.message || '').toLowerCase();
  if (message.includes('quota') && (message.includes('exceeded') || message.includes('billing') || message.includes('plan'))) {
    return true;
  }
  
  return false;
};

/**
 * Detects if an OpenAI error is rate-limit related (temporary, can retry)
 * Based on OpenAI API error codes: https://platform.openai.com/docs/guides/error-codes/api-errors
 */
const isRateLimitError = (status, errorData) => {
  const errorType = errorData?.error?.type || errorData?.error?.code;
  const errorCode = errorData?.error?.code;
  
  // Official OpenAI error codes for rate limits
  if (errorType === 'rate_limit_exceeded') return true;
  if (errorCode === 'rate_limit_exceeded') return true;
  
  // 429 status without insufficient_quota is typically a rate limit
  if (status === 429 && !isQuotaError(status, errorData)) return true;
  
  // Check error message for rate limit keywords (but not quota)
  const message = (errorData?.error?.message || errorData?.message || '').toLowerCase();
  if (message.includes('rate limit') && !message.includes('quota') && !message.includes('billing')) {
    return true;
  }
  
  return false;
};

/**
 * Detects if an OpenAI error is authentication-related
 * Based on OpenAI API error codes: https://platform.openai.com/docs/guides/error-codes/api-errors
 */
const isAuthError = (status, errorData) => {
  const errorType = errorData?.error?.type || errorData?.error?.code;
  const errorCode = errorData?.error?.code;
  
  // Official OpenAI error codes for authentication
  if (errorType === 'invalid_api_key') return true;
  if (errorCode === 'invalid_api_key') return true;
  if (errorType === 'authentication_error') return true;
  if (errorCode === 'authentication_error') return true;
  
  // Status codes
  if (status === 401) return true;
  
  // Check error message
  const message = (errorData?.error?.message || errorData?.message || '').toLowerCase();
  if (message.includes('api key') || message.includes('authentication') || message.includes('unauthorized')) {
    return true;
  }
  
  return false;
};

/**
 * Detects if an OpenAI error is invalid request-related
 * Based on OpenAI API error codes: https://platform.openai.com/docs/guides/error-codes/api-errors
 */
const isInvalidRequestError = (status, errorData) => {
  const errorType = errorData?.error?.type || errorData?.error?.code;
  const errorCode = errorData?.error?.code;
  
  // Official OpenAI error codes for invalid requests
  if (errorType === 'invalid_request_error') return true;
  if (errorCode === 'invalid_request_error') return true;
  
  // Status codes
  if (status === 400) return true;
  
  return false;
};

/**
 * Extracts retry-after information from response headers or error
 */
const extractRetryAfter = (response, errorData) => {
  // Check Retry-After header
  const retryAfterHeader = response?.headers?.get?.('retry-after');
  if (retryAfterHeader) {
    const seconds = parseInt(retryAfterHeader, 10);
    if (!isNaN(seconds)) return seconds;
  }
  
  // Check error message for retry information
  const message = (errorData?.error?.message || errorData?.message || '').toLowerCase();
  const retryMatch = message.match(/retry.*?(\d+)\s*(?:second|minute|hour)/i);
  if (retryMatch) {
    return parseInt(retryMatch[1], 10);
  }
  
  return null;
};

/**
 * Sends a structured error response, with special handling for different OpenAI error types
 * Based on OpenAI API error codes: https://platform.openai.com/docs/guides/error-codes/api-errors
 */
const sendChatError = (res, status, code, message, requestId, meta = {}) => {
  const errorData = meta.errorData || {};
  const isQuota = isQuotaError(status, errorData);
  const isRateLimit = isRateLimitError(status, errorData);
  const isAuth = isAuthError(status, errorData);
  const isInvalidRequest = isInvalidRequestError(status, errorData);
  
  // Extract the original OpenAI error message
  const originalMessage = errorData?.error?.message || message || 'An error occurred';
  const errorType = errorData?.error?.type || errorData?.error?.code || code;
  
  if (isQuota) {
    // Quota exceeded = billing issue, user needs to check their OpenAI account
    // Use the exact OpenAI error message
    return res.status(status).json({
      ok: false,
      errorType: 'quota',
      status,
      code: 'insufficient_quota',
      message: originalMessage, // Use exact OpenAI error message
      requestId,
      retryAfter: null, // Quota errors don't have retry-after
      meta: {
        ...meta,
        quotaExceeded: true,
        actionable: 'Check OpenAI billing dashboard at https://platform.openai.com/usage',
        originalError: errorData?.error, // Include full error for debugging
      },
    });
  }
  
  if (isRateLimit) {
    // Rate limit = temporary, can retry after delay
    const retryAfter = extractRetryAfter(meta.response, errorData);
    // Use the exact OpenAI error message, but add retry info if available
    const rateLimitMessage = retryAfter 
      ? `${originalMessage} Please wait ${retryAfter} seconds before trying again.`
      : originalMessage;
    return res.status(status).json({
      ok: false,
      errorType: 'rate_limit',
      status,
      code: 'rate_limit_exceeded',
      message: rateLimitMessage,
      requestId,
      retryAfter,
      meta: {
        ...meta,
        rateLimited: true,
        actionable: 'Wait and retry',
        originalError: errorData?.error, // Include full error for debugging
      },
    });
  }
  
  if (isAuth) {
    // Authentication error = API key issue
    return res.status(status).json({
      ok: false,
      errorType: 'authentication',
      status,
      code: errorType || 'invalid_api_key',
      message: originalMessage, // Use exact OpenAI error message
      requestId,
      meta: {
        ...meta,
        authenticationError: true,
        actionable: 'Check OPENAI_API_KEY environment variable',
        originalError: errorData?.error, // Include full error for debugging
      },
    });
  }
  
  if (isInvalidRequest) {
    // Invalid request = bad parameters or malformed request
    return res.status(status).json({
      ok: false,
      errorType: 'invalid_request',
      status,
      code: errorType || 'invalid_request_error',
      message: originalMessage, // Use exact OpenAI error message
      requestId,
      meta: {
        ...meta,
        invalidRequest: true,
        actionable: 'Check request parameters and format',
        originalError: errorData?.error, // Include full error for debugging
      },
    });
  }
  
  // For all other errors, return the exact OpenAI error message
  return res.status(status).json({
    ok: false,
    errorType: 'unknown',
    code: errorType || code || 'unknown_error',
    message: originalMessage, // Use exact OpenAI error message
    requestId,
    status,
    meta: {
      ...meta,
      originalError: errorData?.error, // Include full error for debugging
    },
  });
};

const handleChatRequest = async (req, res) => {
  const requestId = req.headers['x-request-id'] || randomUUID();
  const startTime = Date.now();
  const {
    messages,
    model,
    temperature,
    maxTokens,
    agent,
    userId,
    chatId,
    stream,
  } = req.body || {};

    if (!Array.isArray(messages) || messages.length === 0) {
    return sendChatError(res, 400, 'invalid_request', 'messages array is required', requestId);
    }

    const trimmedMessages = messages
      .map(msg => ({
        role: ['system', 'assistant', 'user'].includes(msg.role) ? msg.role : 'user',
      content: typeof msg.content === 'string' ? msg.content.trim() : '',
      }))
    .filter(msg => msg.content.length > 0)
    .slice(-12);

    if (trimmedMessages.length === 0) {
    return sendChatError(res, 400, 'empty_messages', 'All messages were empty after trimming.', requestId);
    }

    // Determine model to use (with fallback support)
    let modelName = model || DEFAULT_CHAT_MODEL;
    let isFallbackAttempt = false;
    const originalModel = modelName;
  const outboundUrl = `${OPENAI_BASE_URL}/chat/completions`;

  logChat('info', requestId, 'request.start', {
    userId: userId || 'unknown',
    chatId: chatId || 'unknown',
    model: modelName,
    url: outboundUrl,
  });

    const makeRequest = async (currentModel) => {
      const payload = {
        model: currentModel,
        messages: [
          { role: 'system', content: buildSystemPrompt(agent) },
        ...trimmedMessages,
        ],
        temperature: typeof temperature === 'number' ? temperature : 0.7,
        max_tokens: Math.min(
          typeof maxTokens === 'number' ? maxTokens : 700,
          parseInt(process.env.OPENAI_MAX_OUTPUT_TOKENS || '900', 10)
        ),
      stream: stream !== false,
      };

      const controller = new AbortController();
      const timeout = setTimeout(() => controller.abort(), CHAT_TIMEOUT_MS);

      try {
        const response = await fetch(outboundUrl, {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
        Authorization: `Bearer ${process.env.OPENAI_API_KEY}`,
        ...(process.env.OPENAI_ORG ? { 'OpenAI-Organization': process.env.OPENAI_ORG } : {}),
          },
          body: JSON.stringify(payload),
      signal: controller.signal,
        });
        clearTimeout(timeout);

        const latency = Date.now() - startTime;

        if (!response.ok) {
          const errorText = await response.text();
          const parsed = safeJsonParse(errorText);
          
          // Log quota and rate limit errors specifically
          if (isQuotaError(response.status, parsed)) {
            logChat('error', requestId, 'request.quota_error', {
              status: response.status,
              latencyMs: latency,
              model: currentModel,
              errorType: parsed?.error?.type || 'unknown',
              errorCode: parsed?.error?.code || 'unknown',
              message: parsed?.error?.message || errorText.slice(0, 200),
              userId: userId || 'unknown',
              chatId: chatId || 'unknown',
              actionable: 'User needs to check OpenAI billing dashboard',
            });
          } else if (isRateLimitError(response.status, parsed)) {
            logChat('warn', requestId, 'request.rate_limit', {
              status: response.status,
              latencyMs: latency,
              model: currentModel,
              errorType: parsed?.error?.type || 'unknown',
              errorCode: parsed?.error?.code || 'unknown',
              message: parsed?.error?.message || errorText.slice(0, 200),
              userId: userId || 'unknown',
              chatId: chatId || 'unknown',
              retryAfter: extractRetryAfter(response, parsed),
            });
          } else {
            logChat('error', requestId, 'request.error', {
              status: response.status,
              latencyMs: latency,
              message: errorText,
              userId: userId || 'unknown',
              chatId: chatId || 'unknown',
            });
          }
          
          return {
            ok: false,
            response,
            parsed,
            errorText,
            latency,
          };
        }

        return {
          ok: true,
          response,
          latency,
        };
      } catch (error) {
        clearTimeout(timeout);
        throw error;
      }
    };

    try {
      // First attempt with requested/default model
      let result = await makeRequest(modelName);
      
      // If rate limit error and fallback enabled, try fallback model (quota errors won't benefit from fallback)
      if (!result.ok && ENABLE_MODEL_FALLBACK && isRateLimitError(result.response.status, result.parsed)) {
        if (modelName !== FALLBACK_MODEL) {
          logChat('warn', requestId, 'request.fallback_attempt', {
            originalModel: modelName,
            fallbackModel: FALLBACK_MODEL,
            reason: 'rate_limit',
            userId: userId || 'unknown',
            chatId: chatId || 'unknown',
          });
          
          modelName = FALLBACK_MODEL;
          isFallbackAttempt = true;
          result = await makeRequest(modelName);
        }
      }

      // Handle error response
      if (!result.ok) {
        const parsed = result.parsed || {};
        const message =
          parsed?.error?.message ||
          parsed?.message ||
          result.errorText?.slice(0, 500) ||
          `Provider error (${result.response.status})`;
        
        return sendChatError(
          res,
          result.response.status,
          'provider_error',
          message,
          requestId,
          {
            latencyMs: result.latency,
            status: result.response.status,
            errorData: parsed,
            response: result.response,
            originalModel,
            fallbackAttempted: isFallbackAttempt,
          }
        );
      }

      // Success - stream the response
      const response = result.response;
      logChat('info', requestId, 'request.response', {
        status: response.status,
        latencyMs: result.latency,
        model: modelName,
        fallbackUsed: isFallbackAttempt,
        userId: userId || 'unknown',
        chatId: chatId || 'unknown',
      });

      res.status(200);
      res.setHeader('Content-Type', 'text/event-stream');
      res.setHeader('Cache-Control', 'no-cache');
      res.setHeader('Connection', 'keep-alive');
      res.setHeader('X-Request-Id', requestId);
      if (isFallbackAttempt) {
        res.setHeader('X-Model-Fallback', 'true');
        res.setHeader('X-Original-Model', originalModel);
      }
      res.flushHeaders?.();

      const decoder = new TextDecoder();
      try {
        for await (const chunk of response.body) {
          res.write(decoder.decode(chunk, { stream: true }));
        }
        res.write(decoder.decode());
        res.write('data: [DONE]\n\n');
        res.end();
      } catch (streamError) {
        logChat('error', requestId, 'request.stream_error', {
          latencyMs: Date.now() - startTime,
          message: streamError.message,
          stack: streamError.stack,
        });
        if (!res.headersSent) {
          return sendChatError(res, 502, 'stream_error', 'Failed to stream provider response', requestId);
        }
        res.end();
      }
  } catch (error) {
    clearTimeout(timeout);
    const latency = Date.now() - startTime;
    const status = error.name === 'AbortError' ? 504 : 502;
    const code = error.name === 'AbortError' ? 'timeout' : 'network_error';
    logChat('error', requestId, 'request.failure', {
      status,
      latencyMs: latency,
      message: error.message,
      stack: error.stack,
    });
    if (!res.headersSent) {
      return sendChatError(
        res,
        status,
        code,
        error.name === 'AbortError' ? 'Provider request timed out' : 'Network error calling provider',
        requestId,
        { latencyMs: latency }
      );
    }
    res.end();
  }
};

app.post('/api/chat', handleChatRequest);

// Content Inspiration Agent Endpoint
app.post('/api/agent/content-inspiration', async (req, res) => {
  const requestId = req.headers['x-request-id'] || randomUUID();
  const startTime = Date.now();
  const { userUploadsSummary, businessIndustry } = req.body || {};

  if (!process.env.OPENAI_API_KEY) {
    return sendChatError(res, 500, 'missing_api_key', 'OpenAI API key not configured', requestId);
  }

  // Build prompt for content strategist agent
  const systemPrompt = `You are a Social Media Content Strategist. Your role is to generate exactly 3 creative, actionable content ideas for social media based on the user's past content and business industry.

You must return ONLY a valid JSON array with exactly 3 objects, each with this exact structure:
{
  "title": "Idea Title (short, catchy)",
  "shortDescription": "1-2 sentences describing the content idea",
  "recommendedPlatform": "Platform name (e.g., Instagram Reels, TikTok, YouTube Shorts, LinkedIn post, Twitter/X, Facebook)"
}

Return ONLY the JSON array, no other text, no markdown, no code blocks. Example format:
[{"title":"...","shortDescription":"...","recommendedPlatform":"..."},{"title":"...","shortDescription":"...","recommendedPlatform":"..."},{"title":"...","shortDescription":"...","recommendedPlatform":"..."}]`;

  let userPrompt = 'Generate 3 creative content ideas for social media.\n\n';
  
  if (businessIndustry) {
    userPrompt += `Business Industry/Niche: ${businessIndustry}\n\n`;
  }
  
  if (userUploadsSummary && userUploadsSummary.length > 0) {
    userPrompt += `Based on the user's past uploads:\n`;
    userUploadsSummary.slice(0, 10).forEach((upload, i) => {
      userPrompt += `${i + 1}. ${upload.title || 'Untitled'} (${upload.type || 'content'})`;
      if (upload.platforms && upload.platforms.length > 0) {
        userPrompt += ` - Platforms: ${upload.platforms.join(', ')}`;
      }
      userPrompt += '\n';
    });
    userPrompt += '\n';
  } else {
    userPrompt += 'No past uploads available. Generate ideas based on the industry only.\n\n';
  }
  
  userPrompt += 'Generate 3 unique, actionable content ideas that would work well for this user.';

  const payload = {
    model: DEFAULT_CHAT_MODEL,
    messages: [
      { role: 'system', content: systemPrompt },
      { role: 'user', content: userPrompt }
    ],
    temperature: 0.7,
    max_tokens: 1000,
    stream: false, // We need structured JSON, not streaming
  };

  try {
    const response = await fetch(`${OPENAI_BASE_URL}/chat/completions`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        Authorization: `Bearer ${process.env.OPENAI_API_KEY}`,
        ...(process.env.OPENAI_ORG ? { 'OpenAI-Organization': process.env.OPENAI_ORG } : {}),
      },
      body: JSON.stringify(payload),
    });

    const latency = Date.now() - startTime;

    if (!response.ok) {
      const errorText = await response.text();
      const parsed = safeJsonParse(errorText);
      logChat('error', requestId, 'content-inspiration.error', {
        status: response.status,
        latencyMs: latency,
        error: parsed?.error || errorText.slice(0, 200),
      });
      return sendChatError(res, response.status, parsed?.error?.code || 'api_error', parsed?.error?.message || 'Failed to generate content ideas', requestId, { errorData: parsed });
    }

    const result = await response.json();
    const content = result.choices?.[0]?.message?.content || '';

    // Parse JSON from response (handle markdown code blocks if present)
    let ideas = [];
    try {
      // Try to extract JSON array from response
      const jsonMatch = content.match(/\[[\s\S]*\]/);
      if (jsonMatch) {
        ideas = JSON.parse(jsonMatch[0]);
      } else {
        ideas = JSON.parse(content);
      }

      // Validate structure
      if (!Array.isArray(ideas) || ideas.length < 3) {
        logChat('warn', requestId, 'content-inspiration.invalid_response', {
          receivedCount: ideas.length,
          content: content.slice(0, 200),
        });
        // Generate fallback ideas
        ideas = generateFallbackIdeas(businessIndustry);
      } else {
        // Ensure each idea has required fields
        ideas = ideas.slice(0, 3).map((idea, index) => ({
          title: idea.title || `Content Idea ${index + 1}`,
          shortDescription: idea.shortDescription || idea.description || 'A creative content idea for your audience.',
          recommendedPlatform: idea.recommendedPlatform || idea.platform || 'Instagram',
        }));
      }
    } catch (parseError) {
      logChat('error', requestId, 'content-inspiration.parse_error', {
        error: parseError.message,
        content: content.slice(0, 200),
      });
      // Generate fallback ideas
      ideas = generateFallbackIdeas(businessIndustry);
    }

    logChat('info', requestId, 'content-inspiration.success', {
      latencyMs: latency,
      ideasCount: ideas.length,
    });

    return res.json({
      success: true,
      ideas,
      requestId,
    });

  } catch (error) {
    const latency = Date.now() - startTime;
    logChat('error', requestId, 'content-inspiration.exception', {
      error: error.message,
      latencyMs: latency,
    });
    return sendChatError(res, 500, 'server_error', `Failed to generate content ideas: ${error.message}`, requestId);
  }
});

// Helper function to generate fallback ideas
function generateFallbackIdeas(industry) {
  const industryContext = industry || 'business';
  return [
    {
      title: 'Educational Content Series',
      shortDescription: `Create a series of educational posts that teach your ${industryContext} audience something valuable. Share tips, insights, and actionable advice.`,
      recommendedPlatform: 'Instagram Reels',
    },
    {
      title: 'Behind-the-Scenes Content',
      shortDescription: 'Share behind-the-scenes content to build authenticity and trust. Show your process, team, or day-to-day operations.',
      recommendedPlatform: 'TikTok',
    },
    {
      title: 'Community Engagement Campaign',
      shortDescription: 'Launch a campaign that encourages audience participation. Ask questions, run polls, or create user-generated content challenges.',
      recommendedPlatform: 'LinkedIn post',
    },
  ];
}
app.post('/api/chat/completions', (req, res) => {
  logChat('warn', req.headers['x-request-id'] || randomUUID(), 'deprecated_endpoint', {
    path: '/api/chat/completions',
  });
  return handleChatRequest(req, res);
});

// Video Analysis Agent Endpoint
app.post('/api/video/analyze', async (req, res) => {
  const requestId = req.headers['x-request-id'] || randomUUID();
  const startTime = Date.now();
  const { transcript } = req.body || {};

  // Safety check: transcript must be provided
  if (!transcript || typeof transcript !== 'string' || transcript.trim().length === 0) {
    return res.status(400).json({
      success: false,
      error: 'Transcript is required and must be a non-empty string',
      requestId,
    });
  }

  if (!process.env.OPENAI_API_KEY) {
    return sendChatError(res, 500, 'missing_api_key', 'OpenAI API key not configured', requestId);
  }

  // VideoAnalysisAgent: Expert video content analyst and script editor
  const systemPrompt = `You are an expert video content analyst and script editor specializing in deep content analysis and script rewriting. Your role is to analyze video transcripts and provide comprehensive insights with actionable improvements.

When analyzing a transcript, you must:
1. Analyze tone, clarity, and pacing
2. Identify 3 key strengths of the content
3. Identify 3 improvement areas
4. Rewrite the script with better flow while preserving the user's speaking style
5. Detect and remove filler words
6. Produce clean, structured output

You MUST respond with ONLY valid JSON. Do not include any text before or after the JSON. Start your response with { and end with }.`;

  const userPrompt = `Analyze this video transcript and provide a comprehensive analysis:

TRANSCRIPT:
${transcript}

Provide your analysis in this exact JSON structure:
{
  "summary": "Comprehensive overview of the video content, main themes, and overall message (2-3 paragraphs)",
  "keyPoints": ["Key insight 1", "Key insight 2", "Key insight 3", "Key insight 4", "Key insight 5"],
  "improvements": [
    "Improvement area 1: Specific recommendation with reasoning",
    "Improvement area 2: Specific recommendation with reasoning",
    "Improvement area 3: Specific recommendation with reasoning"
  ],
  "revisedScript": "Complete rewritten script with improved structure, better pacing, clearer messaging, enhanced engagement elements, and filler words removed. Maintain the user's original speaking style and tone. Make it flow naturally and be engaging.",
  "rawTranscript": "${transcript.substring(0, 1000)}..."
}

Important:
- The summary should be comprehensive and insightful
- Key points should be the main takeaways (5 points)
- Improvements should be specific, actionable recommendations
- Revised script should be a complete rewrite that's better than the original while maintaining the user's voice
- Preserve the original speaking style and tone
- Remove filler words (um, uh, like, you know, etc.)
- Make the script flow naturally and be more engaging`;

  const payload = {
    model: 'gpt-4o', // Use GPT-4o for better analysis
    messages: [
      { role: 'system', content: systemPrompt },
      { role: 'user', content: userPrompt }
    ],
    temperature: 0.7,
    max_tokens: 4000,
    stream: false, // We need structured JSON, not streaming
  };

  try {
    const response = await fetch(`${OPENAI_BASE_URL}/chat/completions`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        Authorization: `Bearer ${process.env.OPENAI_API_KEY}`,
        ...(process.env.OPENAI_ORG ? { 'OpenAI-Organization': process.env.OPENAI_ORG } : {}),
      },
      body: JSON.stringify(payload),
    });

    const latency = Date.now() - startTime;

    if (!response.ok) {
      const errorText = await response.text();
      const parsed = safeJsonParse(errorText);
      logChat('error', requestId, 'video-analyze.error', {
        status: response.status,
        latencyMs: latency,
        error: parsed?.error || errorText.slice(0, 200),
      });
      return sendChatError(res, response.status, parsed?.error?.code || 'api_error', parsed?.error?.message || 'Failed to analyze video transcript', requestId, { errorData: parsed });
    }

    const result = await response.json();
    const content = result.choices?.[0]?.message?.content || '';

    // Parse JSON from response (handle markdown code blocks if present)
    let analysis = null;
    try {
      // Try to extract JSON from response
      let jsonContent = content.trim();
      
      // If the response starts with text before JSON, extract just the JSON part
      const jsonStart = jsonContent.indexOf('{');
      const jsonEnd = jsonContent.lastIndexOf('}');
      
      if (jsonStart !== -1 && jsonEnd !== -1 && jsonEnd > jsonStart) {
        jsonContent = jsonContent.substring(jsonStart, jsonEnd + 1);
      }
      
      analysis = JSON.parse(jsonContent);

      // Validate required fields
      if (!analysis.summary || !analysis.keyPoints || !analysis.improvements || !analysis.revisedScript) {
        throw new Error('Missing required fields in analysis response');
      }

      // Ensure arrays are arrays
      if (!Array.isArray(analysis.keyPoints)) {
        analysis.keyPoints = [analysis.keyPoints];
      }
      if (!Array.isArray(analysis.improvements)) {
        analysis.improvements = [analysis.improvements];
      }

    } catch (parseError) {
      logChat('error', requestId, 'video-analyze.parse_error', {
        error: parseError.message,
        content: content.slice(0, 500),
      });
      
      // Return fallback analysis
      analysis = {
        summary: 'Video analysis completed. The transcript has been processed and analyzed for content structure, clarity, and engagement opportunities.',
        keyPoints: [
          'Content structure and main themes identified',
          'Key messages and takeaways extracted',
          'Engagement opportunities identified',
          'Script flow and pacing analyzed',
          'Improvement areas identified'
        ],
        improvements: [
          'Consider adding a stronger opening hook to grab attention',
          'Improve transitions between main points for better flow',
          'Enhance the call-to-action to be more specific and actionable'
        ],
        revisedScript: transcript, // Fallback to original transcript
        rawTranscript: transcript.substring(0, 1000) + '...',
      };
    }

    logChat('info', requestId, 'video-analyze.success', {
      latencyMs: latency,
      transcriptLength: transcript.length,
    });

    return res.json({
      success: true,
      ...analysis,
      requestId,
    });

  } catch (error) {
    const latency = Date.now() - startTime;
    logChat('error', requestId, 'video-analyze.exception', {
      error: error.message,
      latencyMs: latency,
    });
    
    // Return fallback analysis on error
    return res.json({
      success: true,
      summary: 'Video analysis completed. The transcript has been processed and analyzed.',
      keyPoints: [
        'Content structure analyzed',
        'Key messages identified',
        'Engagement opportunities found',
        'Script flow evaluated',
        'Improvements suggested'
      ],
      improvements: [
        'Consider improving the opening to grab attention',
        'Enhance transitions for better flow',
        'Strengthen the call-to-action'
      ],
      revisedScript: transcript,
      rawTranscript: transcript.substring(0, 1000) + '...',
      requestId,
      fallback: true,
    });
  }
});

// Descript API Proxy Endpoint (kept for backward compatibility, but not used)
app.post('/api/descript/generate-short', upload.single('video'), async (req, res) => {
  const requestId = req.headers['x-request-id'] || randomUUID();
  const startTime = Date.now();

  try {
    if (!req.file) {
      return res.status(400).json({
        success: false,
        error: 'No video file provided'
      });
    }

    // Return placeholder response (Descript API not used in this version)
    res.json({
      success: true,
      requestId,
      message: 'Short generation will use placeholder logic',
      fallback: true,
    });

  } catch (error) {
    console.error(`[descript:${requestId}] error`, error);
    return res.status(500).json({
      success: false,
      error: error.message || 'Failed to generate short video',
      requestId,
    });
  }
});

// Start server
app.listen(PORT, () => {
  console.log('üöÄ Voice Chat Backend Server Started!');
  console.log(`üì° Server running on http://localhost:${PORT}`);
  console.log(`üé§ Transcription endpoint: http://localhost:${PORT}/api/transcribe`);
  console.log(`üí¨ Chat endpoint: http://localhost:${PORT}/api/chat`);
  console.log(`üé¨ Video Analysis endpoint: http://localhost:${PORT}/api/video/analyze`);
  console.log(`üìπ Descript endpoint: http://localhost:${PORT}/api/descript/generate-short`);
  console.log(`‚ù§Ô∏è  Transcription health: http://localhost:${PORT}/api/transcribe/health`);
  console.log(`ü©∫ Chat health: http://localhost:${PORT}/api/chat/health`);
  console.log('');
  console.log('üìù Make sure to set OPENAI_API_KEY in your .env file');
  console.log('üìù VideoAnalysisAgent uses GPT-4o for deep content analysis');
});

module.exports = app;
